{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-TGiQscak_1T","executionInfo":{"status":"ok","timestamp":1729463276155,"user_tz":-120,"elapsed":22117,"user":{"displayName":"Javier A. Espinosa-Oviedo","userId":"06191493186639102551"}}},"outputs":[],"source":["%%capture\n","\n","!pip install -q pyspark findspark\n","\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"]},{"cell_type":"code","source":["!git clone https://github.com/javieraespinosa/big-data-analytics-practicals\n","!mv big-data-analytics-practicals/data/* .\n","!ls *.csv\n"],"metadata":{"id":"hmSXNBepSPmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWPH6P_ku8Zu"},"outputs":[],"source":["spark"]},{"cell_type":"markdown","metadata":{"id":"-be3DYD0k-hN"},"source":["# Introduction au Dataframe avec Spark\n","\n","\n","## 1 Présentation\n","\n","Pour faciliter le développement de programme, Spark à partir de la version 2 intégre la notion de Dataframe.\n","\n","Le DataFrame est une couche d'abstraction des RDD, qui présentent les données comme des tables de base de données sans se préoccuper de la taille des données.\n","Les DataFrames offrent de nombreux avantages:\n","* Une syntaxe beaucoup plus simple.\n","* Possibilité d'utiliser SQL directement dans la trame de données.\n","* La parallélisation des traitements dans une architecture distribuée est gérée par Spark.\n","\n","Si vous avez utilisé R ou même la bibliothèque pandas avec Python, vous connaissez probablement déjà le concept des DataFrames. Même si Spark supporte plusieurs langages il faut savoir que le langage le moins efficace est le python.\n","\n","Vous êtes prêt, jouons un peu avec les dataframes.\n","\n","Let's get started!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HdJDLKACk-hV"},"source":["## 1 - Les fichiers de données"]},{"cell_type":"markdown","metadata":{"id":"bX5LRzDfk-hV"},"source":["Pour lire un fichier, il suffit d'utiliser la fonction associée à votre format de fichier:\n","* csv,txt -> load\n","* parquet -> parquet\n","* json -> json\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoIvFAYVk-hW"},"outputs":[],"source":["df_csv = spark.read.load(\"sales_info.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"]},{"cell_type":"markdown","metadata":{"id":"6v1viQCdk-hW"},"source":["Dans les options de lecture du fichier, nous avons demandé à Spark d'utiliser le séparteur \",\"  et d'inférer le schéma.\n","\n","Pour consulter le schéma évaluer par Spark utilisez la fonction printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sJcgTAVk-hW"},"outputs":[],"source":["df_csv.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"OMKxYuark-hX"},"source":["Pour voir les fonctions disponibles avec les dataframes consultez la documentation Spark :\n","https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\n","\n","Pour avoir un aperçu des données, utilisez la méthode show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZhIA-ZHk-hX"},"outputs":[],"source":["df_csv.show()"]},{"cell_type":"markdown","metadata":{"id":"acytOv04k-hX"},"source":["Vous l'aurez compris Spark utilise un échantillonage des données pour évaluer le schema des données.\n","\n","Si le schéma ne vous convient pas vous pouvez lui indiquer le schema que vous souhaitez (renommer les noms des colonnes par exemple)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gj97dvSk-hY"},"outputs":[],"source":["from pyspark.sql.types import *\n","# Required for StructField, StringType, IntegerType, etc.\n","\n","\n","fields = [  StructField(\"Compagnie\", StringType(), True),\n","            StructField(\"Personne\", StringType(), True),\n","            StructField(\"Ventes\", FloatType(), False)\n","            ]\n","\n","csvSchema = StructType(fields)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ag3ObPYhk-hY"},"outputs":[],"source":["df_csv_with_my_schema = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").schema(csvSchema).csv(\"sales_info.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3E-8SI-hk-hY"},"outputs":[],"source":["df_csv_with_my_schema.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nx7dg89rk-hZ","tags":[]},"outputs":[],"source":["df_csv_with_my_schema.show()"]},{"cell_type":"markdown","metadata":{"id":"4vgD4byXk-hZ"},"source":["Les fichiers csv et texte ne sont pas des formats adaptés pour de grosse volumétrie.\n","Une bonne pratique dans les environnements de BIG DATA est de travailler avec des formats binaires pour 2 raisons :\n","* il consomme moins de stockage.\n","* il est plus facile de déplacer des données via le réseau.\n","\n","Dans les écosystèmes HADOOP, les données sont sérialisées sous 2 formes :\n","* Ligne : Avro, Kryo, Protobuff\n","* Column : Parquet, Orc\n","\n","En entreprise, vous trouverez souvent les formats Avro ou parquet pour leur performance.\n","\n","Pour convertir un fichier en parquet c'est aussi simple que ça :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65FzyrREk-hZ"},"outputs":[],"source":["df_csv_with_my_schema.write.format(\"parquet\").save(\"sales_info.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDGkBrq7k-ha"},"outputs":[],"source":["!ls sales_info.parquet/"]},{"cell_type":"markdown","metadata":{"id":"BNFiihXJk-ha"},"source":["Vous pouvez remarquer qu'au format parquet, notre fichier sales_info.parquet est un répertoire contenant des fichiers part-00000, cela vous rappelle-t'il quelque chose ?\n","En effet, pour traiter des données parquet sait que les données vont être distribuées et par conséquent applique le principe de partitionnement. Pour un fichier de plusieurs Go, il est possible d'indiquer la taille des partitions parquet.\n","\n","Bien entendu, la plupart des formats binaires inscrit le schéma des données dans chaque partition.\n","\n","Vérifions cela :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L5to_3Vk-hb"},"outputs":[],"source":["Sales_parquet = spark.read.parquet(\"sales_info.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_Paj-kSk-hb"},"outputs":[],"source":["Sales_parquet.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D5jlUPIk-hb"},"outputs":[],"source":["Sales_parquet.show()"]},{"cell_type":"markdown","metadata":{"id":"WUipd3Ark-hb"},"source":["On peut facilement convertir d'un format à un autre :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MterkxxSk-hc"},"outputs":[],"source":["Sales_parquet.write.format('json').save('sales.json')"]},{"cell_type":"markdown","metadata":{"id":"Jf4GU9J1k-hc"},"source":["Pour lire ensuite vos données."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2MUmMHRk-hd"},"outputs":[],"source":["Sales_json = spark.read.json(\"sales.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgwrwnFxk-he"},"outputs":[],"source":["Sales_json.show()"]},{"cell_type":"markdown","metadata":{"id":"zeBoDqrfk-hf"},"source":["# 2 Exploiter les DataFrames"]},{"cell_type":"markdown","metadata":{"id":"gnumh0_Vk-hf"},"source":["Pour manipuler les DataFrames, nous avons un ensemble de méthodes disponibles :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pc63FW-Xk-hg","tags":[]},"outputs":[],"source":["df = spark.read.parquet(\"sales_info.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1NBb835k-hg"},"outputs":[],"source":["dir(df)"]},{"cell_type":"markdown","metadata":{"id":"2sr0RdhRk-hg"},"source":["Pour afficher les colonnes :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cV-GwFB4k-hg","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["df.columns"]},{"cell_type":"markdown","metadata":{"id":"bB6W9cg7k-hh"},"source":["Travaillons à présent sur les manipulations que nous pouvons faire :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQeQmy8Gk-hh","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["dfColVentes = df.select('Ventes')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vzu4vxH3k-hi"},"outputs":[],"source":["type(dfColVentes)"]},{"cell_type":"markdown","metadata":{"id":"gcf9BVthk-hi"},"source":["Le résultat retourné par le select est un DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsDDuQEuk-hi","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["df.select('Ventes').show()"]},{"cell_type":"markdown","metadata":{"id":"D2Oicv6Lk-hj"},"source":["Pour selectionner les N premières lignes, on trouvera la fonction head :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51Xr3xD3k-hj","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# Retourne une d'object RowReturns list of Row objects\n","df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"hPqqgqTqk-hj"},"source":["Pour sélectionner multiple colonne :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEtTH8MBk-hj","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["df.select(['Compagnie','Ventes']).show()"]},{"cell_type":"markdown","metadata":{"id":"awWP6R8Ak-hk"},"source":["ou"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMVTPZ7pk-hk"},"outputs":[],"source":["df.select(df.Compagnie.alias('test'), df.Ventes).show()"]},{"cell_type":"markdown","metadata":{"id":"OOB6Fafwk-hk"},"source":["Pour le renommage d'une colonne on peut utiliser la méthode alias :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FIWiYtBk-hk"},"outputs":[],"source":["df.select(df.Compagnie.alias('col1'), df.Ventes.alias('col2')).show()"]},{"cell_type":"markdown","metadata":{"id":"QuX2cuqtk-hl"},"source":["Si on veut créer de nouvelles colonnes à partir d'une colonne existante  :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1YyGvmYk-hl","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["df.withColumn('VentesAdd', df['Ventes'] * 10).show()"]},{"cell_type":"markdown","metadata":{"id":"SIMpiqyhk-hl"},"source":["Pour renommer une colonne"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-_q9Ulnk-hl","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# Simple Rename\n","df.withColumnRenamed('Compagnie', 'Company').show()"]},{"cell_type":"markdown","metadata":{"id":"sGshnLngk-hl"},"source":["On peut filtrer sur un critére :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Pyz5rF_k-hm"},"outputs":[],"source":["df.filter(df.Personne==\"Sam\").show()"]},{"cell_type":"markdown","metadata":{"id":"KJ7gFUFkk-hn"},"source":["Pour les opérations d'agrégation, on utilisera les fonctions du module pyspark.sql.functions lequel regroupe les fonctions max, min, mean ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pq2j3eOjk-hn"},"outputs":[],"source":["import pyspark.sql.functions as f\n","\n","df.groupBy('Compagnie').agg(f.max('Ventes').alias('Max Ventes'), f.min('Ventes').alias('Min Ventes'), f.mean('Ventes').alias('Moy_Ventes')).orderBy('Compagnie').show()"]},{"cell_type":"markdown","metadata":{"id":"-3JLhzCZk-hn"},"source":["Comme pour les bases de données relationnelles, on peut faire des jointures entre des sources de données avec différents formats :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUHCvSXsk-hn"},"outputs":[],"source":["from pyspark.sql import *\n","# Création d'un dataframe des localisations des différentes compagnies.\n","# Le code ci-dessous indique comment créer un dataframe manuellement\n","\n","Location = Row(\"Compagnie\", \"location\")\n","location1 = Location('GOOG', 'US')\n","location2 = Location('MSFT', 'EUROPE')\n","location3 = Location('FB', 'ASIA')\n","\n","locationRows = [location1, location2, location3]\n","df_location  = spark.createDataFrame(locationRows)\n","type(df_location)"]},{"cell_type":"markdown","metadata":{"id":"cjhFtVBuk-ho"},"source":["On affiche le contenu de notre nouveau dataframe :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsaZqfwCk-ho"},"outputs":[],"source":["df_location.show()"]},{"cell_type":"markdown","metadata":{"id":"eBA_uYsmk-ho"},"source":["On affiche la structure du dataframe :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MH0Fl5yCk-ho"},"outputs":[],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"DnmxZ949k-ho"},"source":["On réalise la jointure entre le dataframe df listant toutes les ventes des companies et le dataframe df_location précisant la localisation des companies :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7Ctiezbk-hp"},"outputs":[],"source":["df.join(df_location, df_location.Compagnie == df.Compagnie, \"left_outer\").show()"]},{"cell_type":"markdown","metadata":{"id":"5gqEHb8-k-hp"},"source":["### Utilisation du SQL\n","\n","Pour utiliser des requêtes SQL directement sur un DataFrame, vous devrez l'enregistrer dans une vue temporaire:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLvU0G6Mk-hp","tags":[]},"outputs":[],"source":["# La méthode createOrReplaceTempView enregistre\n","# le DataFrame comme une view temporaire\n","df.createOrReplaceTempView(\"sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEb2LoEXk-hp","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sql_sales = spark.sql(\"SELECT * FROM sales\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIhDhiEpk-hq","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sql_sales"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erECoIxKk-hq","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sql_sales.show()"]},{"cell_type":"markdown","metadata":{"id":"SqUb86kyk-hq"},"source":["Bien entendu, vous pouvez appliquer des requêtes SQL avec des conditions :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcC1YrBLk-hq","jupyter":{"outputs_hidden":false}},"outputs":[],"source":["spark.sql(\"SELECT * FROM sales WHERE Ventes > 500\").show()"]},{"cell_type":"markdown","metadata":{"id":"f7DuareQk-hq"},"source":["Spark SQL supporte un sous ensemble de la norme SQL92."]},{"cell_type":"markdown","metadata":{"id":"cbJKjBz_k-hr"},"source":["Bravo !!!\n","\n","Vous êtes prêt à faire vos premiers exercices avec Spark."]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"nbpresent":{"slides":{},"themes":{"default":"0535adbc-b74f-46cc-9cd6-4eabe2477c8e","theme":{"0535adbc-b74f-46cc-9cd6-4eabe2477c8e":{"backgrounds":{"backgroundColor":{"background-color":"backgroundColor","id":"backgroundColor"}},"id":"0535adbc-b74f-46cc-9cd6-4eabe2477c8e","palette":{"backgroundColor":{"id":"backgroundColor","rgb":[43,43,43]},"headingColor":{"id":"headingColor","rgb":[238,238,238]},"linkColor":{"id":"linkColor","rgb":[19,218,236]},"mainColor":{"id":"mainColor","rgb":[238,238,238]}},"rules":{"a":{"color":"linkColor"},"h1":{"color":"headingColor","font-family":"Oswald","font-size":7},"h2":{"color":"headingColor","font-family":"Oswald","font-size":5},"h3":{"color":"headingColor","font-family":"Oswald","font-size":3.75},"h4":{"color":"headingColor","font-family":"Oswald","font-size":3},"h5":{"color":"headingColor","font-family":"Oswald"},"h6":{"color":"headingColor","font-family":"Oswald"},"h7":{"color":"headingColor","font-family":"Oswald"},"li":{"color":"mainColor","font-family":"Lato","font-size":5},"p":{"color":"mainColor","font-family":"Lato","font-size":5}},"text-base":{"color":"mainColor","font-family":"Lato","font-size":5}},"cc59980f-cb69-400a-b63a-1fb85ca73c8a":{"backgrounds":{"dc7afa04-bf90-40b1-82a5-726e3cff5267":{"background-color":"31af15d2-7e15-44c5-ab5e-e04b16a89eff","id":"dc7afa04-bf90-40b1-82a5-726e3cff5267"}},"id":"cc59980f-cb69-400a-b63a-1fb85ca73c8a","palette":{"19cc588f-0593-49c9-9f4b-e4d7cc113b1c":{"id":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","rgb":[252,252,252]},"31af15d2-7e15-44c5-ab5e-e04b16a89eff":{"id":"31af15d2-7e15-44c5-ab5e-e04b16a89eff","rgb":[68,68,68]},"50f92c45-a630-455b-aec3-788680ec7410":{"id":"50f92c45-a630-455b-aec3-788680ec7410","rgb":[197,226,245]},"c5cc3653-2ee1-402a-aba2-7caae1da4f6c":{"id":"c5cc3653-2ee1-402a-aba2-7caae1da4f6c","rgb":[43,126,184]},"efa7f048-9acb-414c-8b04-a26811511a21":{"id":"efa7f048-9acb-414c-8b04-a26811511a21","rgb":[25.118061674008803,73.60176211453744,107.4819383259912]}},"rules":{"a":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c"},"blockquote":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-size":3},"code":{"font-family":"Anonymous Pro"},"h1":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Merriweather","font-size":8},"h2":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Merriweather","font-size":6},"h3":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-family":"Lato","font-size":5.5},"h4":{"color":"c5cc3653-2ee1-402a-aba2-7caae1da4f6c","font-family":"Lato","font-size":5},"h5":{"font-family":"Lato"},"h6":{"font-family":"Lato"},"h7":{"font-family":"Lato"},"li":{"color":"50f92c45-a630-455b-aec3-788680ec7410","font-size":3.25},"pre":{"font-family":"Anonymous Pro","font-size":4}},"text-base":{"color":"19cc588f-0593-49c9-9f4b-e4d7cc113b1c","font-family":"Lato","font-size":4}}}}}},"nbformat":4,"nbformat_minor":0}